{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8beb10d",
   "metadata": {},
   "source": [
    "### Overview of the typical steps involved in using a Recurrent Neural Network (RNN) for sequence generation:\n",
    "\n",
    "1. Data Preparation: Collect and preprocess the dataset.\n",
    "\n",
    "2. Model Architecture: Choose RNN variant (e.g., LSTM), define layers, units, and additional components.\n",
    "\n",
    "3. Training: Split dataset, define loss function, compile model, train iteratively.\n",
    "\n",
    "4. Sequence Generation: Seed generation, iteratively predict next element in sequence.\n",
    "\n",
    "5. Evaluation: Assess quality of generated sequences using appropriate metrics.\n",
    "\n",
    "6. Deployment: Deploy trained model, monitor performance, update as needed\n",
    "\n",
    "\n",
    "## LSTM (Long Short-Term Memory) is a type of RNN (Recurrent Neural Network) designed to address the issue of capturing long-term dependencies in sequential data.\n",
    "- LSTMs have a memory cell that can retain information over long periods.\n",
    "- They incorporate gating mechanisms (input, forget, and output gates) to control the flow of information into and out of the memory cell.\n",
    "- Input gate: Determines which new information to incorporate into the memory cell.\n",
    "- Forget gate: Decides which information to discard from the memory cell.\n",
    "- Output gate: Regulates the information output from the memory cell to the next time step.\n",
    "- LSTMs are trained using backpropagation through time (BPTT), allowing them to learn to capture long-range dependencies in the data.\n",
    "- LSTMs are widely used in tasks such as language modeling, speech recognition, and time series prediction due to their ability to effectively capture long-range dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21afd760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed sequence: [1, 2, 3, 4, 5]\n",
      "Generated seed: [3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "## Code for Seed Generation\n",
    "import numpy as np\n",
    "\n",
    "# Define the seed sequence\n",
    "seed_sequence = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Function to generate seed from sequence\n",
    "def generate_seed(sequence, seed_length):\n",
    "    seed = sequence[-seed_length:]\n",
    "    return seed\n",
    "\n",
    "# Generate seed\n",
    "seed_length = 3\n",
    "seed = generate_seed(seed_sequence, seed_length)\n",
    "\n",
    "# Display seed\n",
    "print(\"Seed sequence:\", seed_sequence)\n",
    "print(\"Generated seed:\", seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751a1728",
   "metadata": {},
   "source": [
    "### A simple Python code example using a recurrent neural network (RNN) to generate a sequence:\n",
    "- We generate a synthetic dataset where each input sequence consists of random integers, and the corresponding output is the sum of those integers.\n",
    "\n",
    "- We define an RNN model using Keras, consisting of an LSTM layer followed by a dense layer.\n",
    "\n",
    "- The model is compiled with the Adam optimizer and mean squared error loss.\n",
    "\n",
    "- We reshape the input data to fit the LSTM input shape.\n",
    "\n",
    "- The model is trained on the dataset.\n",
    "\n",
    "- Finally, we generate a new sequence of random integers, reshape it, and use the trained model to predict the sum of the new sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ddb4727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 2s 4ms/step - loss: 253995.4375\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 252477.1562\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 250682.0156\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 248517.2812\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 246160.2500\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 244021.8594\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 242245.0781\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 240959.2656\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 239769.5625\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 238631.9375\n",
      "1/1 [==============================] - 0s 329ms/step\n",
      "Generated sequence sum: 17.250885\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Define the dataset\n",
    "sequence_length = 10\n",
    "num_samples = 1000\n",
    "X = np.random.randint(0, 100, (num_samples, sequence_length))\n",
    "y = np.sum(X, axis=1)\n",
    "\n",
    "# Define the RNN model\n",
    "model = Sequential([\n",
    "    LSTM(32, input_shape=(sequence_length, 1)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Reshape input data for LSTM\n",
    "X = np.reshape(X, (num_samples, sequence_length, 1))\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=10, batch_size=32)\n",
    "\n",
    "# Generate a new sequence\n",
    "new_sequence_input = np.random.randint(0, 100, (1, sequence_length))\n",
    "new_sequence_input = np.reshape(new_sequence_input, (1, sequence_length, 1))\n",
    "predicted_value = model.predict(new_sequence_input)\n",
    "\n",
    "print(\"Generated sequence sum:\", predicted_value[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4629d4f",
   "metadata": {},
   "source": [
    "##  Python code example demonstrating a sampling strategy known as greedy decoding:\n",
    "- The greedy_decode() function takes a list of probabilities (predictions) and an optional temperature parameter.\n",
    "- It applies temperature scaling to the predictions to control the randomness of the sampling.\n",
    "- The function then selects the index with the highest probability as the sampled index.\n",
    "- The sampled index is returned as the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c1da9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled index using greedy decoding: 3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a function for greedy decoding\n",
    "def greedy_decode(predictions, temperature=1.0):\n",
    "    # Apply temperature scaling to the predictions\n",
    "    predictions = np.log(predictions) / temperature\n",
    "    exp_preds = np.exp(predictions)\n",
    "    predictions = exp_preds / np.sum(exp_preds)\n",
    "    \n",
    "    # Choose the index of the highest probability as the next element\n",
    "    sampled_index = np.argmax(predictions)\n",
    "    return sampled_index\n",
    "\n",
    "# Example usage\n",
    "predictions = np.array([0.1, 0.2, 0.3, 0.4])  # Example probabilities\n",
    "sampled_index = greedy_decode(predictions)\n",
    "print(\"Sampled index using greedy decoding:\", sampled_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10397942",
   "metadata": {},
   "source": [
    "## Implement and train an LSTM model using Python with the TensorFlow library\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abba43cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4/4 [==============================] - 1s 2ms/step - loss: 0.2769\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2095\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1552\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1186\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 0s/step - loss: 0.0996\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 0s/step - loss: 0.0950\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 167us/step - loss: 0.0958\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 0s/step - loss: 0.0962\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0957\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.0952\n",
      "1/1 [==============================] - 0s 275ms/step\n",
      "Predicted value: [[0.57015204]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Generate some dummy data for demonstration\n",
    "# Input shape: (samples, time steps, features)\n",
    "# Output shape: (samples, features)\n",
    "data = np.random.rand(100, 10, 1)  # 100 samples, 10 time steps, 1 feature\n",
    "target = np.random.rand(100, 1)    # 100 samples, 1 feature\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(32, input_shape=(10, 1)),  # 32 LSTM units, input shape: (10, 1)\n",
    "    Dense(1)                        # Output layer with 1 neuron\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')  # Mean Squared Error loss\n",
    "\n",
    "# Train the model\n",
    "model.fit(data, target, epochs=10, batch_size=32)\n",
    "\n",
    "# Once trained, you can use the model for predictions\n",
    "# For example, predict using the first sample from the data\n",
    "sample = data[0].reshape(1, 10, 1)  # Reshape to (1, 10, 1) for single sample prediction\n",
    "prediction = model.predict(sample)\n",
    "print(\"Predicted value:\", prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa35b47a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
